{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4vpRvg1_kaI"
      },
      "source": [
        "# Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9vGiQ7W_TEa"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers[torch] transformers accelerate image-reward clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "DZdSogYe_uE7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from PIL import Image\n",
        "import json\n",
        "from io import BytesIO\n",
        "import os\n",
        "from functools import partialmethod\n",
        "from PIL import Image\n",
        "\n",
        "from diffusers import DiffusionPipeline, EulerDiscreteScheduler\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import ImageReward as RM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shj79zHO_uQJ"
      },
      "source": [
        "# Constant Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "hGBCkpQz_wkP"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DTYPE = torch.float16\n",
        "\n",
        "TURBO_URL = \"stabilityai/sd-turbo\"\n",
        "# TURBO_URL = \"stabilityai/sdxl-turbo\"\n",
        "\n",
        "SD_URL = \"stabilityai/stable-diffusion-2-1-base\"\n",
        "\n",
        "CLIP_URL = \"openai/clip-vit-large-patch14\"\n",
        "REWARD_URL = \"ImageReward-v1.0\"\n",
        "\n",
        "LATENT_SHAPE = lambda bs: (bs, 4, 64, 64)\n",
        "\n",
        "BASE_PATH = \"./drive/MyDrive/RewardSearch/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEk9A5Aw_20i"
      },
      "source": [
        "# Model Initialiation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8MKJbwVABju"
      },
      "outputs": [],
      "source": [
        "def get_turbo_pipeline():\n",
        "  scheduler = EulerDiscreteScheduler.from_pretrained(TURBO_URL, subfolder=\"scheduler\")\n",
        "  pipeline = DiffusionPipeline.from_pretrained(\n",
        "      TURBO_URL, torch_dtype=torch.float16, variant=\"fp16\",\n",
        "      scheduler=scheduler\n",
        "  ).to(\"cpu\")\n",
        "\n",
        "  _ = torch.compile(pipeline, mode=\"reduce-overhead\", fullgraph=True)\n",
        "\n",
        "  return pipeline\n",
        "\n",
        "\n",
        "def get_sd_pipeline():\n",
        "  scheduler = EulerDiscreteScheduler.from_pretrained(SD_URL, subfolder=\"scheduler\")\n",
        "  pipeline = DiffusionPipeline.from_pretrained(\n",
        "      SD_URL, torch_dtype=torch.float16, variant=\"fp16\",\n",
        "      scheduler=scheduler\n",
        "  ).to(\"cpu\")\n",
        "\n",
        "  _ = torch.compile(pipeline, mode=\"reduce-overhead\", fullgraph=True)\n",
        "\n",
        "  return pipeline\n",
        "\n",
        "\n",
        "def get_clip():\n",
        "  clip_processor = CLIPProcessor.from_pretrained(CLIP_URL)\n",
        "  clip_model = CLIPModel.from_pretrained(\n",
        "      CLIP_URL, torch_dtype=DTYPE, variant=\"fp16\",\n",
        "      from_tf=True\n",
        "  ).to(\"cpu\")\n",
        "\n",
        "  _ = torch.compile(clip_model, mode=\"reduce-overhead\", fullgraph=True)\n",
        "\n",
        "  return clip_processor, clip_model\n",
        "\n",
        "\n",
        "def get_reward_model():\n",
        "  model = RM.load(REWARD_URL).to(\"cpu\")\n",
        "\n",
        "  _ = torch.compile(model, mode=\"reduce-overhead\", fullgraph=True)\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "turbo_pipeline = get_turbo_pipeline()\n",
        "sd_pipeline = get_sd_pipeline()\n",
        "clip_processor, clip_model = get_clip()\n",
        "reward_model = get_reward_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVqYrUvIAAZr"
      },
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "5AsY87wx03Z3"
      },
      "outputs": [],
      "source": [
        "class DotDict(dict):\n",
        "  \"\"\" A dictionary that allows item = d.key access for brevity. \"\"\"\n",
        "\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init__(*args)\n",
        "\n",
        "    for k, v in kwargs.items():\n",
        "      self[k] = v\n",
        "\n",
        "\n",
        "  def copy(self):\n",
        "    return DotDict(self)\n",
        "\n",
        "\n",
        "  def from_dict(self, d, recursive=False):\n",
        "    for k, v in d.items():\n",
        "\n",
        "      if recursive and isinstance(v, dict):\n",
        "        self[k] = DotDict().from_dict(v, recursive=True)\n",
        "\n",
        "      elif recursive and (isinstance(v, list) or (isinstance(v, np.ndarray) and isinstance(v[0], dict))):\n",
        "        self[k] = []\n",
        "        for it in v:\n",
        "          if isinstance(it, dict):\n",
        "            self[k].append(DotDict().from_dict(it, recursive=True))\n",
        "          else:\n",
        "            self[k].append(it)\n",
        "\n",
        "      else:\n",
        "        self[k] = v\n",
        "\n",
        "    return self\n",
        "\n",
        "  def to_dict(self, recursive=False):\n",
        "    d = dict()\n",
        "    for k, v in self.items():\n",
        "\n",
        "      if recursive and isinstance(v, dict):\n",
        "        d[k] = DotDict().to_dict(v, recursive=True)\n",
        "\n",
        "      elif recursive and (isinstance(v, list) or (isinstance(v, np.ndarray) and isinstance(v[0], dict))):\n",
        "        d[k] = []\n",
        "        for it in v:\n",
        "          if isinstance(it, dict):\n",
        "            d[k].append(DotDict().to_dict(it, recursive=True))\n",
        "          else:\n",
        "            d[k].append(it)\n",
        "\n",
        "      else:\n",
        "        d[k] = v\n",
        "\n",
        "    return d\n",
        "\n",
        "\n",
        "  def __getattr__(self, k):\n",
        "    try:\n",
        "      return super().__getattr__(k)\n",
        "    except AttributeError:\n",
        "      return self[k]\n",
        "\n",
        "\n",
        "  def __setattr__(self, k, v):\n",
        "    try:\n",
        "      super().__getattr__(k)\n",
        "      super().__setattr__(k, v)\n",
        "    except AttributeError:\n",
        "      self[k] = v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "jXltXHKt-swE"
      },
      "outputs": [],
      "source": [
        "class Cudize:\n",
        "  \"\"\" To save GPU RAM, temporarily move models to gpu,\n",
        "  and enable torch.inference_mode().\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, *args):\n",
        "    self.models = [m for m in args]\n",
        "    self.inference_manager = None\n",
        "\n",
        "  def __enter__(self):\n",
        "    for m in self.models:\n",
        "      m.to(DEVICE)\n",
        "    self.inference_manager = torch.inference_mode(True)\n",
        "    self.inference_manager.__enter__()\n",
        "    return self\n",
        "\n",
        "  def __exit__(self, *args):\n",
        "    for m in self.models:\n",
        "      m.to(\"cpu\")\n",
        "    self.inference_manager.__exit__(*args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "R8kMW-PbcHU0"
      },
      "outputs": [],
      "source": [
        "class Silence:\n",
        "  \"\"\" Silence tqdm progress bars, including inside of packages. \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __enter__(self):\n",
        "    self.tmp__init__ = tqdm.__init__\n",
        "    tqdm.__init__ = partialmethod(tqdm.__init__, disable=True)\n",
        "    return self\n",
        "\n",
        "  def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "    tqdm.__init__ = self.tmp__init__\n",
        "    if exc_type is not None:\n",
        "      raise exc_type(exc_val, exc_tb)\n",
        "    return self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "23BsUkahGPiN"
      },
      "outputs": [],
      "source": [
        "def slerp(val, low, high):\n",
        "    \"\"\" Batched spherical interpolation between high and low.\n",
        "    val in [0, 1], 0=low, 1=high.\n",
        "    \"\"\"\n",
        "\n",
        "    assert low.shape == high.shape\n",
        "    og_shape = low.shape\n",
        "\n",
        "    low = low.reshape(low.shape[0], -1)\n",
        "    high = high.reshape(high.shape[0], -1)\n",
        "\n",
        "    low_norm = low/torch.norm(low, dim=1, keepdim=True)\n",
        "    high_norm = high/torch.norm(high, dim=1, keepdim=True)\n",
        "    omega = torch.acos((low_norm*high_norm).sum(1))\n",
        "    so = torch.sin(omega)\n",
        "    res = (torch.sin((1.0-val)*omega)/so).unsqueeze(1)*low + (torch.sin(val*omega)/so).unsqueeze(1) * high\n",
        "\n",
        "    return res.reshape(og_shape)\n",
        "\n",
        "\n",
        "def slap(theta, x, y):\n",
        "  \"\"\" Multidimensional slerp to rotate a vector 360 degrees on a 2d plane.\n",
        "  theta in radians\n",
        "  \"\"\"\n",
        "  theta = theta % (2*np.pi)\n",
        "\n",
        "  if theta < np.pi/2:\n",
        "    val = theta / (np.pi/2)\n",
        "    return slerp(val, x, y)\n",
        "\n",
        "  elif theta < np.pi:\n",
        "    val = (theta - np.pi/2) / (np.pi/2)\n",
        "    return slerp(val, y, -x)\n",
        "\n",
        "  elif theta < 3*np.pi/2:\n",
        "    val = (theta - np.pi) / (np.pi/2)\n",
        "    return slerp(val, -x, -y)\n",
        "\n",
        "  val = (theta - 3*np.pi/2) / (np.pi/2)\n",
        "  return slerp(val, -y, x)\n",
        "\n",
        "\n",
        "def slink(thetas, basis):\n",
        "  \"\"\" Generalization of slap to any number of dimensions.\n",
        "  Can represent a vector in a k dimensional basis using k-1 angles.\n",
        "  thetas in radians\n",
        "  \"\"\"\n",
        "  assert len(thetas) == len(basis)-1\n",
        "\n",
        "  out = basis[0][None]\n",
        "  for i in range(len(thetas)):\n",
        "    out = slap(thetas[i], out, basis[i+1][None])\n",
        "\n",
        "  return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CutIl7ENAFut"
      },
      "source": [
        "# Scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "F6VnpZdbHZOQ"
      },
      "outputs": [],
      "source": [
        "class Scorer:\n",
        "  \"\"\" Wrapper for various scoring functions \"\"\"\n",
        "\n",
        "  def __init__(self, reward_model, clip_processor, clip_model):\n",
        "    self.reward_model = reward_model\n",
        "    self.clip_processor = clip_processor\n",
        "    self.clip_model = clip_model\n",
        "\n",
        "    self.buffer = BytesIO()\n",
        "\n",
        "    self.clip_text_cache = None\n",
        "    self.clip_prompt_cache = None\n",
        "\n",
        "\n",
        "  def reward_score(self, prompt, images):\n",
        "    # ImageReward\n",
        "    out = self.reward_model.inference_rank(prompt, images)[1]\n",
        "    if isinstance(out, list):\n",
        "      return out\n",
        "    return [out]\n",
        "\n",
        "\n",
        "  def clip_score(self, prompt, images):\n",
        "    # CLIP Score\n",
        "\n",
        "    if self.clip_text_cache is None or prompt != self.clip_prompt_cache:\n",
        "\n",
        "      inputs = self.clip_processor(text=[prompt], return_tensors=\"pt\").to(self.clip_model.device)\n",
        "      text_emb = clip_model.get_text_features(**inputs)\n",
        "\n",
        "      text_emb /= torch.norm(text_emb, dim=-1, keepdim=True)\n",
        "\n",
        "      self.clip_text_cache = text_emb\n",
        "      self.clip_prompt_cache = prompt\n",
        "\n",
        "    else:\n",
        "\n",
        "      text_emb = self.clip_text_cache\n",
        "\n",
        "    inputs = self.clip_processor(images=images, return_tensors=\"pt\").to(self.clip_model.device)\n",
        "    img_emb = clip_model.get_image_features(**inputs)\n",
        "\n",
        "    img_emb /= torch.norm(img_emb, dim=-1, keepdim=True)\n",
        "\n",
        "    return torch.sum(text_emb * img_emb, dim=-1).detach().cpu().numpy().tolist()\n",
        "\n",
        "\n",
        "  def jpeg_score(self, prompt, images):\n",
        "    # JPEG compression ratio\n",
        "\n",
        "    out = []\n",
        "    for im in images:\n",
        "\n",
        "      buffer = BytesIO()\n",
        "      im.save(buffer, \"JPEG\")\n",
        "      b = len(buffer.getvalue())\n",
        "\n",
        "      out.append(im.width * im.height * 3 / b)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "scorer = Scorer(\n",
        "  reward_model,\n",
        "  clip_processor,\n",
        "  clip_model,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0o0UBjnANEj"
      },
      "source": [
        "# Model Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "kj0H1JhGMrWr"
      },
      "outputs": [],
      "source": [
        "def get_prompt_embeds(prompt, pipeline, batch_size=1, use_guidance=False):\n",
        "  # Embed prompts once for speed\n",
        "  return pipeline.encode_prompt(\n",
        "      prompt=prompt,\n",
        "      device=pipeline.unet.device,\n",
        "      num_images_per_prompt=batch_size,\n",
        "      do_classifier_free_guidance=use_guidance,\n",
        "  )\n",
        "\n",
        "\n",
        "def generate_images_turbo(prompt, pipeline, num_inference_steps, latents=None, prompt_embeds=None, batch_size=1, output_type=\"pil\"):\n",
        "  # Wrapper for generation with SD-Turbo\n",
        "\n",
        "  with Silence():\n",
        "    if prompt_embeds is None:\n",
        "      return pipeline(\n",
        "          prompt=prompt,\n",
        "          num_inference_steps=num_inference_steps,\n",
        "          num_images_per_prompt=batch_size,\n",
        "          latents=latents,\n",
        "          output_type=output_type,\n",
        "          guidance_scale=0.0,\n",
        "      ).images\n",
        "\n",
        "    if len(prompt_embeds) == 2:\n",
        "      return pipeline(\n",
        "          prompt=prompt,\n",
        "          num_inference_steps=num_inference_steps,\n",
        "          num_images_per_prompt=batch_size,\n",
        "          latents=latents,\n",
        "          output_type=output_type,\n",
        "          guidance_scale=0.0,\n",
        "          prompt_embeds=prompt_embeds[0],\n",
        "          negative_prompt_embeds=prompt_embeds[1],\n",
        "      ).images\n",
        "\n",
        "    return pipeline(\n",
        "          prompt=prompt,\n",
        "          num_inference_steps=num_inference_steps,\n",
        "          num_images_per_prompt=batch_size,\n",
        "          latents=latents,\n",
        "          output_type=output_type,\n",
        "          guidance_scale=0.0,\n",
        "          prompt_embeds=prompt_embeds[0],\n",
        "          negative_prompt_embeds=prompt_embeds[1],\n",
        "          pooled_prompt_embeds=prompt_embeds[2],\n",
        "          negative_pooled_prompt_embeds=prompt_embeds[3],\n",
        "      ).images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpGFu-QtBmpd"
      },
      "source": [
        "# Search Algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "AfY0LhPcDbMS"
      },
      "outputs": [],
      "source": [
        "def gradient_ascent(\n",
        "    prompt,\n",
        "    pipeline,\n",
        "    score_fn,\n",
        "    num_steps,\n",
        "    num_inference_steps,\n",
        "    step_size\n",
        "):\n",
        "  \"\"\" Gradient ascent/descent with a fixed m.\n",
        "  Becomes Random Sampling with step_size=1.\n",
        "\n",
        "  Args:\n",
        "    prompt: image prompt\n",
        "    pipeline: diffusers pipeline\n",
        "    score_fn: (prompt, images) -> list(scores)\n",
        "    num_steps: number of steps to search\n",
        "    num_inference_steps: steps per inference rollout\n",
        "    step_size: interpolation value m\n",
        "\n",
        "  Returns\n",
        "    scores: array of scores per step\n",
        "    images: image for each step\n",
        "    acc_steps: steps where the new state was accepted\n",
        "    rej_steps: steps where the new state was rejected\n",
        "  \"\"\"\n",
        "\n",
        "  prompt_embeds = get_prompt_embeds(prompt, pipeline, batch_size=1)\n",
        "\n",
        "  curr_latents = torch.randn(LATENT_SHAPE(1), device=DEVICE, dtype=DTYPE)\n",
        "  curr_img = generate_images_turbo(None, pipeline, num_inference_steps, latents=curr_latents, prompt_embeds=prompt_embeds)\n",
        "  curr_score = score_fn(prompt, curr_img)[0]\n",
        "\n",
        "  scores = [curr_score]\n",
        "  images = [np.array(curr_img[0])]\n",
        "  acc_steps = [0]\n",
        "  rej_steps = [0]\n",
        "\n",
        "  for step in (pbar:=tqdm(range(1, 1+num_steps))):\n",
        "\n",
        "    basis = torch.randn(LATENT_SHAPE(1), device=DEVICE, dtype=DTYPE)\n",
        "    new_latents = slerp(step_size, curr_latents, basis)\n",
        "\n",
        "    new_img = generate_images_turbo(None, pipeline, num_inference_steps, latents=new_latents, prompt_embeds=prompt_embeds)\n",
        "    new_score = score_fn(prompt, new_img)[0]\n",
        "\n",
        "    if new_score >= curr_score:\n",
        "      curr_latents = new_latents\n",
        "      curr_img = new_img\n",
        "      curr_score = new_score\n",
        "\n",
        "      acc_steps.append(step)\n",
        "    else:\n",
        "      rej_steps.append(step)\n",
        "\n",
        "    scores.append(new_score)\n",
        "    images.append(np.array(new_img[0]))\n",
        "\n",
        "    pbar.set_postfix(curr_score=f\"{curr_score:.3f}\", new_score=f\"{new_score:.3f}\")\n",
        "\n",
        "  return DotDict(\n",
        "    scores=np.array(scores),\n",
        "    images=np.stack(images),\n",
        "    acc_steps=np.array(acc_steps),\n",
        "    rej_steps=np.array(rej_steps)\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "Jy5ArS7wJzgf"
      },
      "outputs": [],
      "source": [
        "def simulated_annealing(\n",
        "    prompt,\n",
        "    pipeline,\n",
        "    score_fn,\n",
        "    num_steps,\n",
        "    num_inference_steps,\n",
        "    mutation_start,\n",
        "    mutation_decay,\n",
        "    temp_start,\n",
        "    temp_decay,\n",
        "):\n",
        "  \"\"\" Simulated Annealing algorithm.\n",
        "  Becomes Stochastic Hill Climbing with temp_start -> 0.\n",
        "\n",
        "  Args:\n",
        "    prompt: image prompt\n",
        "    pipeline: diffusers pipeline\n",
        "    score_fn: (prompt, images) -> list(scores)\n",
        "    num_steps: number of steps to search\n",
        "    num_inference_steps: steps per inference rollout\n",
        "    mutation_start: initial m\n",
        "    mutation_decay: m decay rate\n",
        "    temp_start: initial temperature\n",
        "    temp_decay: temperature decay rate.\n",
        "\n",
        "  Returns\n",
        "    scores: array of scores per step\n",
        "    images: image for each step\n",
        "    acc_steps: steps where the new state was accepted\n",
        "    rej_steps: steps where the new state was rejected\n",
        "  \"\"\"\n",
        "\n",
        "  prompt_embeds = get_prompt_embeds(prompt, pipeline, batch_size=1)\n",
        "\n",
        "  curr_latents = torch.randn(LATENT_SHAPE(1), device=DEVICE, dtype=DTYPE)\n",
        "  curr_img = generate_images_turbo(None, pipeline, num_inference_steps, latents=curr_latents, prompt_embeds=prompt_embeds)\n",
        "  curr_score = score_fn(prompt, curr_img)[0]\n",
        "\n",
        "  mut = mutation_start\n",
        "  temp = temp_start\n",
        "\n",
        "  scores = [curr_score]\n",
        "  images = [np.array(curr_img[0])]\n",
        "  acc_steps = [0]\n",
        "  rej_steps = [0]\n",
        "\n",
        "  for step in (pbar:=tqdm(range(1, num_steps+1))):\n",
        "\n",
        "    basis = torch.randn(LATENT_SHAPE(1), device=DEVICE, dtype=DTYPE)\n",
        "    new_latents = slerp(mut, curr_latents, basis)\n",
        "\n",
        "    new_img = generate_images_turbo(None, pipeline, num_inference_steps, latents=new_latents, prompt_embeds=prompt_embeds)\n",
        "    new_score = score_fn(prompt, new_img)[0]\n",
        "\n",
        "    if (\n",
        "        new_score >= curr_score or\n",
        "        np.random.rand() < np.exp((new_score - curr_score)/temp)\n",
        "    ):\n",
        "      curr_latents = new_latents\n",
        "      curr_score = new_score\n",
        "\n",
        "      acc_steps.append(step)\n",
        "    else:\n",
        "      rej_steps.append(step)\n",
        "\n",
        "    scores.append(new_score)\n",
        "    images.append(np.array(new_img[0]))\n",
        "\n",
        "    mut *= mutation_decay\n",
        "    temp *= temp_decay\n",
        "\n",
        "    pbar.set_postfix(curr_score=f\"{curr_score:.3f}\", new_score=f\"{new_score:.3f}\")\n",
        "\n",
        "  return DotDict(\n",
        "    scores=np.array(scores),\n",
        "    images=np.stack(images),\n",
        "    acc_steps=np.array(acc_steps),\n",
        "    rej_steps=np.array(rej_steps)\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91xdrzu7P9pl"
      },
      "source": [
        "# Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "xIXFs8TMP84u"
      },
      "outputs": [],
      "source": [
        "BASE_CONFIG = DotDict(\n",
        "    num_steps=50,\n",
        "    num_inference_steps=2\n",
        ")\n",
        "\n",
        "TEST_CONFIG = DotDict()\n",
        "\n",
        "\n",
        "\"\"\" One-Shot \"\"\"\n",
        "TEST_CONFIG.ONE_SHOT = DotDict(\n",
        "    algorithm=gradient_ascent\n",
        ")\n",
        "\n",
        "TEST_CONFIG.ONE_SHOT.REWARD = DotDict(\n",
        "    score_fn=scorer.reward_score,\n",
        "    num_steps=0,\n",
        "    num_inference_steps=BASE_CONFIG.num_inference_steps,\n",
        "    step_size=1.0\n",
        ")\n",
        "\n",
        "TEST_CONFIG.ONE_SHOT.CLIP = TEST_CONFIG.ONE_SHOT.REWARD.copy()\n",
        "TEST_CONFIG.ONE_SHOT.CLIP.score_fn = scorer.clip_score\n",
        "\n",
        "TEST_CONFIG.ONE_SHOT.JPEG = TEST_CONFIG.ONE_SHOT.REWARD.copy()\n",
        "TEST_CONFIG.ONE_SHOT.JPEG.score_fn = scorer.jpeg_score\n",
        "\n",
        "\n",
        "\"\"\" Random \"\"\"\n",
        "TEST_CONFIG.RANDOM = DotDict(\n",
        "    algorithm=gradient_ascent\n",
        ")\n",
        "\n",
        "TEST_CONFIG.RANDOM.REWARD = DotDict(\n",
        "    score_fn=scorer.reward_score,\n",
        "    num_steps=BASE_CONFIG.num_steps,\n",
        "    num_inference_steps=BASE_CONFIG.num_inference_steps,\n",
        "    step_size=1.0\n",
        ")\n",
        "\n",
        "TEST_CONFIG.RANDOM.CLIP = TEST_CONFIG.RANDOM.REWARD.copy()\n",
        "TEST_CONFIG.RANDOM.CLIP.score_fn = scorer.clip_score\n",
        "\n",
        "TEST_CONFIG.RANDOM.JPEG = TEST_CONFIG.RANDOM.REWARD.copy()\n",
        "TEST_CONFIG.RANDOM.JPEG.score_fn = scorer.jpeg_score\n",
        "\n",
        "\n",
        "\"\"\" Gradient Descent \"\"\"\n",
        "TEST_CONFIG.GRADIENT_DESCENT = DotDict(\n",
        "    algorithm=gradient_ascent\n",
        ")\n",
        "\n",
        "TEST_CONFIG.GRADIENT_DESCENT.REWARD = DotDict(\n",
        "    score_fn=scorer.reward_score,\n",
        "    num_steps=BASE_CONFIG.num_steps,\n",
        "    num_inference_steps=BASE_CONFIG.num_inference_steps,\n",
        "    step_size=0.01\n",
        ")\n",
        "\n",
        "TEST_CONFIG.GRADIENT_DESCENT.CLIP = TEST_CONFIG.GRADIENT_DESCENT.REWARD.copy()\n",
        "TEST_CONFIG.GRADIENT_DESCENT.CLIP.score_fn = scorer.clip_score\n",
        "\n",
        "TEST_CONFIG.GRADIENT_DESCENT.JPEG = TEST_CONFIG.GRADIENT_DESCENT.REWARD.copy()\n",
        "TEST_CONFIG.GRADIENT_DESCENT.JPEG.score_fn = scorer.jpeg_score\n",
        "\n",
        "\n",
        "\"\"\" Simulated Annealing \"\"\"\n",
        "TEST_CONFIG.SIMULATED_ANNEALING = DotDict(\n",
        "    algorithm=simulated_annealing\n",
        ")\n",
        "\n",
        "TEST_CONFIG.SIMULATED_ANNEALING.REWARD = DotDict(\n",
        "    score_fn=scorer.reward_score,\n",
        "    num_steps=BASE_CONFIG.num_steps,\n",
        "    num_inference_steps=BASE_CONFIG.num_inference_steps,\n",
        "    mutation_start=1.0,\n",
        "    mutation_decay=0.92,\n",
        "    temp_start=0.25,\n",
        "    temp_decay=0.92\n",
        ")\n",
        "\n",
        "TEST_CONFIG.SIMULATED_ANNEALING.CLIP = TEST_CONFIG.SIMULATED_ANNEALING.REWARD.copy()\n",
        "TEST_CONFIG.SIMULATED_ANNEALING.CLIP.score_fn = scorer.clip_score\n",
        "TEST_CONFIG.SIMULATED_ANNEALING.CLIP.temp_start=0.025\n",
        "\n",
        "TEST_CONFIG.SIMULATED_ANNEALING.JPEG = TEST_CONFIG.SIMULATED_ANNEALING.REWARD.copy()\n",
        "TEST_CONFIG.SIMULATED_ANNEALING.JPEG.score_fn = scorer.jpeg_score\n",
        "TEST_CONFIG.SIMULATED_ANNEALING.JPEG.temp_start=1.0\n",
        "\n",
        "\n",
        "\"\"\" Greedy Annealing \"\"\"\n",
        "TEST_CONFIG.GREEDY = DotDict(\n",
        "    algorithm=simulated_annealing\n",
        ")\n",
        "\n",
        "TEST_CONFIG.GREEDY.REWARD = DotDict(\n",
        "    score_fn=scorer.reward_score,\n",
        "    num_steps=BASE_CONFIG.num_steps,\n",
        "    num_inference_steps=BASE_CONFIG.num_inference_steps,\n",
        "    mutation_start=1.0,\n",
        "    mutation_decay=0.92,\n",
        "    temp_start=0.0001,\n",
        "    temp_decay=0.92\n",
        ")\n",
        "\n",
        "TEST_CONFIG.GREEDY.CLIP = TEST_CONFIG.GREEDY.REWARD.copy()\n",
        "TEST_CONFIG.GREEDY.CLIP.score_fn = scorer.clip_score\n",
        "\n",
        "TEST_CONFIG.GREEDY.JPEG = TEST_CONFIG.GREEDY.REWARD.copy()\n",
        "TEST_CONFIG.GREEDY.JPEG.score_fn = scorer.jpeg_score\n",
        "\n",
        "\n",
        "\"\"\" Extra Info \"\"\"\n",
        "NAME_TO_SCORE_MODEL = {\n",
        "  \"REWARD\": reward_model,\n",
        "  \"CLIP\": clip_model,\n",
        "  \"JPEG\": nn.Linear(1, 1),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sXr7k8IOPl4"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gy4PG4JYYBx-"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(BASE_PATH, \"benchmark-prompts.json\"), \"r\") as f:\n",
        "  PROMPTS = json.load(f)\n",
        "\n",
        "RESULT_PATH = os.path.join(BASE_PATH, \"results\")\n",
        "os.makedirs(RESULT_PATH, exist_ok=True)\n",
        "\n",
        "\n",
        "\"\"\" Generate predictions for every benchmark prompt\"\"\"\n",
        "for alg_name, alg_config in TEST_CONFIG.items():\n",
        "  if alg_name in [\"ONE_SHOT\", \"RANDOM\", \"GRADIENT_DESCENT\", \"SIMULATED_ANNEALING\"]:\n",
        "    continue\n",
        "\n",
        "  for r_name, r_config in alg_config.items():\n",
        "    if r_name in [\"algorithm\", \"REWARD\"]:\n",
        "      continue\n",
        "\n",
        "    save_path = os.path.join(RESULT_PATH, f\"{alg_name}-{r_name}.npy\")\n",
        "    results = []\n",
        "    final_scores = []\n",
        "\n",
        "    with Cudize(turbo_pipeline, NAME_TO_SCORE_MODEL[r_name]):\n",
        "      for prompt_dict in (pbar:=tqdm(PROMPTS, desc=f\"{alg_name}-{r_name}\")):\n",
        "        id = prompt_dict[\"id\"]\n",
        "        prompt = prompt_dict[\"prompt\"]\n",
        "\n",
        "        torch.manual_seed(0)\n",
        "        torch.cuda.manual_seed(0)\n",
        "        np.random.seed(0)\n",
        "\n",
        "        with Silence():\n",
        "          out = alg_config.algorithm(\n",
        "            prompt,\n",
        "            turbo_pipeline,\n",
        "            **r_config\n",
        "          )\n",
        "        out.pop(\"images\")\n",
        "\n",
        "        results.append({\n",
        "            \"id\": id,\n",
        "            \"prompt\": prompt,\n",
        "            \"result\": out.to_dict(recursive=True)\n",
        "        })\n",
        "        final_scores.append(np.max(out.scores))\n",
        "\n",
        "        pbar.set_postfix(\n",
        "            median=np.median(final_scores),\n",
        "            mean=np.mean(final_scores),\n",
        "            std=np.std(final_scores)\n",
        "        )\n",
        "\n",
        "      np.save(save_path, results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7SsUGERbAaK"
      },
      "outputs": [],
      "source": [
        "def load_database():\n",
        "  \"\"\" Load data generated by above \"\"\"\n",
        "\n",
        "  database = {}\n",
        "\n",
        "  for f in os.listdir(RESULT_PATH):\n",
        "    filename = f[:f.find(\".\")]\n",
        "    if filename == \"\":\n",
        "      continue\n",
        "\n",
        "    alg, reward = tuple(filename.split(\"-\"))\n",
        "\n",
        "    if reward not in database:\n",
        "      database[reward] = {}\n",
        "    database[reward][alg] = np.load(os.path.join(RESULT_PATH, f), allow_pickle=True)\n",
        "\n",
        "  return DotDict().from_dict(database, recursive=True)\n",
        "\n",
        "database = load_database()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02mkAkjRvvQV"
      },
      "outputs": [],
      "source": [
        "COLORS = {\n",
        "    \"RANDOM\": \"k\",\n",
        "    \"SIMULATED_ANNEALING\": \"b\",\n",
        "    \"GREEDY\": \"r\",\n",
        "    \"ONE_SHOT\": \"y\"\n",
        "}\n",
        "\n",
        "AXIS_NAMES = {\n",
        "    \"CLIP\": \"CLIP Score\",\n",
        "    \"JPEG\": \"JPEG Compression Ratio\",\n",
        "    \"REWARD\": \"ImageReward Score\",\n",
        "}\n",
        "\n",
        "ALG_NAMES = {\n",
        "    \"RANDOM\": \"Random\",\n",
        "    \"SIMULATED_ANNEALING\": \"SA\",\n",
        "    \"GREEDY\": \"SHC\",\n",
        "    \"ONE_SHOT\": \"y\"\n",
        "}\n",
        "\n",
        "TITLE_NAMES = {\n",
        "    \"CLIP\": \"CLIP\",\n",
        "    \"JPEG\": \"JPEG Compression Ratio\",\n",
        "    \"REWARD\": \"ImageReward\",\n",
        "}\n",
        "\n",
        "def visualize_reward_curves(database, reward, ax):\n",
        "  \"\"\" Visualize the score curves for each algorithm,\n",
        "  also print the mean performance.\n",
        "  \"\"\"\n",
        "\n",
        "  print(f\"\\n{reward}\")\n",
        "  for alg, alg_data in database[reward].items():\n",
        "    if alg in [\"GRADIENT_DESCENT\"]:\n",
        "      continue\n",
        "\n",
        "    scores = np.stack(r.result.scores for r in alg_data)\n",
        "    infer_scores = np.stack([np.max(scores[:, :i+1], axis=-1) for i in range(scores.shape[1])], axis=-1)\n",
        "\n",
        "    print(f\"\\t{alg}: {np.mean(infer_scores[:,-1]):.4f}\")\n",
        "\n",
        "    ax.plot(\n",
        "        range(len(scores[0])),\n",
        "        np.mean(infer_scores, axis=0),\n",
        "        color=COLORS[alg],\n",
        "        label=ALG_NAMES[alg]\n",
        "    )\n",
        "    # plt.plot(\n",
        "    #     range(len(scores[0])),\n",
        "    #     np.median(infer_scores, axis=0),\n",
        "    #     \":\",\n",
        "    #     color=COLORS[alg],\n",
        "    #     label=alg\n",
        "    # )\n",
        "    # plt.fill_between(\n",
        "    #     range(len(scores[0])),\n",
        "    #     np.mean(infer_scores, axis=0) - np.std(infer_scores, axis=0)/10,\n",
        "    #     np.mean(infer_scores, axis=0) + np.std(infer_scores, axis=0)/10,\n",
        "    #     alpha=0.1,\n",
        "    #     color=COLORS[alg],\n",
        "    # )\n",
        "\n",
        "  print(\"\")\n",
        "\n",
        "  ax.legend()\n",
        "  ax.set_xlabel(\"Search Step\")\n",
        "  ax.set_ylabel(AXIS_NAMES[reward])\n",
        "  ax.set_title(f\"{TITLE_NAMES[reward]} Score\")\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(2, 2, figsize=(8, 8))\n",
        "\n",
        "visualize_reward_curves(database, \"REWARD\", ax[0,0])\n",
        "visualize_reward_curves(database, \"CLIP\", ax[0, 1])\n",
        "visualize_reward_curves(database, \"JPEG\", ax[1, 0])\n",
        "ax[1,1].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rpgcp8xwwUj9"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eavGzvyZIWxS"
      },
      "outputs": [],
      "source": [
        "def get_grid(prompt, pipeline, grid_size, batch_size):\n",
        "  \"\"\"\n",
        "  Generate a grid of 2d interpolated images.\n",
        "  \"\"\"\n",
        "\n",
        "  assert GRID_SIZE % BATCH_SIZE == 0\n",
        "\n",
        "  # torch.manual_seed(0)\n",
        "  # torch.cuda.manual_seed(0)\n",
        "  # np.random.seed(0)\n",
        "\n",
        "  x = torch.randn(LATENT_SHAPE(2), device=DEVICE, dtype=DTYPE)\n",
        "  y = torch.randn(LATENT_SHAPE(2), device=DEVICE, dtype=DTYPE)\n",
        "\n",
        "  grid = np.zeros((grid_size, grid_size, 512, 512, 3))\n",
        "\n",
        "  with Cudize(turbo_pipeline):\n",
        "\n",
        "    prompt_embeds = get_prompt_embeds(prompt, pipeline, batch_size=batch_size)\n",
        "\n",
        "    for col in tqdm(range(grid_size)):\n",
        "\n",
        "      assert grid_size % batch_size == 0\n",
        "      batched_r = []\n",
        "\n",
        "      for batch in range(grid_size // batch_size):\n",
        "        theta = col / grid_size\n",
        "\n",
        "        l1 = slerp(theta, x[:1], x[1:])\n",
        "        l2 = slerp(theta, y[:1], y[1:])\n",
        "\n",
        "        latents = torch.cat(\n",
        "            [slerp(lamb, l1, l2) for lamb in np.linspace(0, 1, grid_size)[batch*batch_size:(1+batch)*batch_size]],\n",
        "            dim=0\n",
        "        )\n",
        "\n",
        "        with Silence():\n",
        "          images = generate_images_turbo(\n",
        "              None,\n",
        "              pipeline,\n",
        "              2,\n",
        "              prompt_embeds=prompt_embeds,\n",
        "              latents=latents,\n",
        "              batch_size=batch_size,\n",
        "              output_type=\"np\"\n",
        "          )\n",
        "\n",
        "        for row in range(BATCH_SIZE):\n",
        "          grid[row+batch_size*batch, col] = images[row]\n",
        "\n",
        "  return grid\n",
        "\n",
        "\n",
        "PROMPT = \"a concept art of a vehicle, cyberpunk\"\n",
        "\n",
        "GRID_SIZE = 33\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "grid = get_grid(PROMPT, turbo_pipeline, GRID_SIZE, BATCH_SIZE)[:-3, :-3]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVqvVjC-ynAt"
      },
      "outputs": [],
      "source": [
        "def get_score_grid(prompt, grid, score_fn, score_model):\n",
        "  \"\"\"\n",
        "  Apply a score function to a grid of images.\n",
        "  \"\"\"\n",
        "\n",
        "  s_grid = np.zeros(grid.shape[:2])\n",
        "\n",
        "  with Cudize(score_model):\n",
        "    for i in tqdm(range(grid.shape[0])):\n",
        "      for j in range(grid.shape[1]):\n",
        "        img = Image.fromarray((grid[i, j] * 255).astype(np.uint8))\n",
        "        score = score_fn(prompt, [img])[0]\n",
        "        s_grid[i, j] = score\n",
        "\n",
        "  return s_grid\n",
        "\n",
        "plt.imshow(grid[0,0])\n",
        "plt.show()\n",
        "\n",
        "reward_grid = get_score_grid(PROMPT, grid, scorer.reward_score, reward_model)\n",
        "plt.matshow(reward_grid)\n",
        "plt.show()\n",
        "\n",
        "clip_grid = get_score_grid(PROMPT, grid, scorer.clip_score, clip_model)\n",
        "plt.matshow(clip_grid)\n",
        "plt.show()\n",
        "\n",
        "jpeg_grid = get_score_grid(PROMPT, grid, scorer.jpeg_score, reward_model)\n",
        "plt.matshow(jpeg_grid)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXCip9s1CPfO"
      },
      "outputs": [],
      "source": [
        "\"\"\" Visualize select images from a grid. \"\"\"\n",
        "\n",
        "fig, ax = plt.subplots(4, 4, figsize=(24, 24))\n",
        "\n",
        "g_plot = grid[::8, ::8]\n",
        "for i in range(4):\n",
        "  for j in range(4):\n",
        "    ax[i, j].imshow(g_plot[i, j])\n",
        "    ax[i, j].axis(\"off\")\n",
        "\n",
        "plt.title(\"2 Dimensional Image Interpolation\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sg257dwGZSJ"
      },
      "outputs": [],
      "source": [
        "\"\"\" Show score grids as a single image. \"\"\"\n",
        "\n",
        "fig, ax = plt.subplots(2, 2, figsize=(6, 6))\n",
        "\n",
        "ax[0,0].matshow(reward_grid)\n",
        "plt.colorbar(ax[0,0].matshow(reward_grid), ax=ax[0,0])\n",
        "ax[0,0].set_title(\"ImageReward Score\")\n",
        "\n",
        "ax[1,0].matshow(jpeg_grid)\n",
        "plt.colorbar(ax[1,0].matshow(jpeg_grid), ax=ax[1,0])\n",
        "ax[1,0].set_title(\"CLIP Score\")\n",
        "\n",
        "ax[0,1].matshow(clip_grid)\n",
        "plt.colorbar(ax[0,1].matshow(clip_grid), ax=ax[0,1])\n",
        "ax[0,1].set_title(\"JPEG Compression Score\")\n",
        "\n",
        "ax[1,1].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2-3HhdGE9hM"
      },
      "outputs": [],
      "source": [
        "\"\"\" Get a before/after search is applied to the prompt.\n",
        "Also visualize the accepted/rejected graph.\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = \"delicious plate of food\"\n",
        "SUFFIX = \"\"\n",
        "\n",
        "SCORE_FUNCTION = scorer.reward_score\n",
        "\n",
        "NUM_STEPS = 50\n",
        "NUM_INFERENCE_STEPS = 2\n",
        "\n",
        "STEP_SIZE = 1.0\n",
        "\n",
        "MUTATION_START = 0.01\n",
        "MUTATION_DECAY = 1.0\n",
        "\n",
        "START_TEMP = 0.000001\n",
        "TEMP_DECAY = 0.92\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "with Cudize(turbo_pipeline, reward_model):\n",
        "  out = simulated_annealing(\n",
        "    f\"{PROMPT}, {SUFFIX}\",\n",
        "    turbo_pipeline,\n",
        "    SCORE_FUNCTION,\n",
        "    NUM_STEPS,\n",
        "    NUM_INFERENCE_STEPS,\n",
        "    MUTATION_START,\n",
        "    MUTATION_DECAY,\n",
        "    START_TEMP,\n",
        "    TEMP_DECAY,\n",
        "    # STEP_SIZE\n",
        "  )\n",
        "\n",
        "plt.scatter(out.rej_steps, out.scores[out.rej_steps], color=\"r\", marker=\"x\", label=\"rejected\")\n",
        "plt.scatter(out.acc_steps, out.scores[out.acc_steps], color=\"b\", label=\"accepted\")\n",
        "plt.plot(out.acc_steps, out.scores[out.acc_steps], color=\"b\")\n",
        "plt.legend()\n",
        "plt.title(f\"Random Sampling Rollout\")\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"ImageReward Score\")\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "ax[0].imshow(out.images[0])\n",
        "ax[0].axis(\"off\")\n",
        "ax[0].set_title(f\"Initial: Score={out.scores[0]:.4f}\")\n",
        "\n",
        "ax[1].imshow(out.images[np.argmax(out.scores)])\n",
        "ax[1].axis(\"off\")\n",
        "ax[1].set_title(f\"Final: Score={out.scores.max():.4f}\")\n",
        "\n",
        "plt.suptitle(\"SHC with Small Fixed m\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h01fnKmPZ8g9"
      },
      "source": [
        "# Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ut52_vuiY41E"
      },
      "outputs": [],
      "source": [
        "\"\"\" Generate large visualizations\n",
        "of images from different scores/algorithms/prompts/models\n",
        "\"\"\"\n",
        "\n",
        "# from benchmark\n",
        "PROMPTS = [\n",
        "    \"a concept art of a vehicle, cyberpunk\",\n",
        "    \"a beautiful portrait of a beautiful woman in the jungle surrounded by pink flowers, shamanism, matte painting, fantasy art\",\n",
        "    \"a cute cat\",\n",
        "     \"close up photo of anthropomorphic fox animal dressed in white shirt and khaki cargo pants, fox animal, glasses\",\n",
        "    \"Portrait of an old sea captain, male, detailed face, fantasy, highly detailed, cinematic, art painting by greg rutkowski\",\n",
        "    \"classic model of atoms, made out of glass marbles and chrome steel rods, studio\",\n",
        "    \"cartoon character in a style of adventure time cartoon\",\n",
        "    \"an alien planet viewed from space, extremely, beautiful, dynamic, creative, cinematic\"\n",
        "]\n",
        "\n",
        "SHOW_NAMES = [\n",
        "    \"One-Shot\",\n",
        "    \"Random Sampling\",\n",
        "    \"Simulated Annealing\",\n",
        "    \"Stochastic Hill Climbing\"\n",
        "]\n",
        "\n",
        "R_NAMES = {\n",
        "    \"REWARD\": \"ImageReward\",\n",
        "    \"CLIP\": \"CLIP\",\n",
        "    \"JPEG\": \"JPEG Compression Ratio\",\n",
        "}\n",
        "\n",
        "for r_name in TEST_CONFIG.ONE_SHOT.keys():\n",
        "  if r_name == \"algorithm\":\n",
        "    continue\n",
        "\n",
        "  fig, ax = plt.subplots(len(PROMPTS), 6, figsize=(4*6, 0.35+4*len(PROMPTS)))\n",
        "\n",
        "  for i, prompt in enumerate(tqdm(PROMPTS)):\n",
        "\n",
        "    text = ax[i, 0].text(0.5, 0.5, prompt, horizontalalignment='center',\n",
        "     verticalalignment='center', size='x-large', wrap=True, transform=ax[i, 0].transAxes)\n",
        "    text._get_wrap_line_width = lambda : 250.\n",
        "    ax[i, 0].axis(\"off\")\n",
        "\n",
        "    to_run = list(TEST_CONFIG.items())\n",
        "    for k in range(len(to_run)):\n",
        "      if to_run[k][0] == \"GRADIENT_DESCENT\":\n",
        "        to_run.pop(k)\n",
        "        break\n",
        "\n",
        "    for j, tup in enumerate(tqdm(to_run, leave=False)):\n",
        "      alg_name, alg_config = tup\n",
        "\n",
        "      with Cudize(turbo_pipeline, NAME_TO_SCORE_MODEL[r_name]):\n",
        "        torch.manual_seed(0)\n",
        "        torch.cuda.manual_seed(0)\n",
        "        np.random.seed(0)\n",
        "        out = alg_config.algorithm(\n",
        "          prompt,\n",
        "          turbo_pipeline,\n",
        "          **alg_config[r_name]\n",
        "        )\n",
        "\n",
        "      im = out.images[out.scores.argmax()]\n",
        "\n",
        "      if i == 0:\n",
        "        ax[i,j+1].set_title(f\"{SHOW_NAMES[j]}\\n\\nScore={out.scores.max():.4f}\")\n",
        "      else:\n",
        "        ax[i,j+1].set_title(f\"Score={out.scores.max():.4f}\")\n",
        "\n",
        "      ax[i,j+1].imshow(im)\n",
        "      ax[i,j+1].axis(\"off\")\n",
        "\n",
        "    with Cudize(sd_pipeline, NAME_TO_SCORE_MODEL[r_name]):\n",
        "      with Silence():\n",
        "        torch.manual_seed(0)\n",
        "        torch.cuda.manual_seed(0)\n",
        "        np.random.seed(0)\n",
        "        im = sd_pipeline(\n",
        "          prompt=prompt\n",
        "        ).images\n",
        "        score = TEST_CONFIG.ONE_SHOT[r_name].score_fn(prompt, im)[0]\n",
        "\n",
        "    if i == 0:\n",
        "      ax[i,-1].set_title(f\"Stable Diffusion 2.1\\n\\nScore={score:.4f}\")\n",
        "    else:\n",
        "      ax[i,-1].set_title(f\"Score={score:.4f}\")\n",
        "\n",
        "    ax[i,-1].imshow(im[0])\n",
        "    ax[i,-1].axis(\"off\")\n",
        "\n",
        "  plt.suptitle(f\"{R_NAMES[r_name]} Optimized Examples\", fontsize=\"xx-large\")\n",
        "  # plt.tight_layout()\n",
        "  plt.show()\n",
        "  plt.clf()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RteFh1FLOSO0"
      },
      "source": [
        "# Junk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsmZvWq7XTri"
      },
      "outputs": [],
      "source": [
        "\n",
        "PROMPT = \"delicious plate of food\"\n",
        "\n",
        "NUM_EPOCHS = 10\n",
        "STEPS_PER_EPOCH = 5\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "MUTATION_START = 1 # 1/2\n",
        "MUTATION_DECAY = 1 # 0.95\n",
        "\n",
        "START_TEMP = 1/2\n",
        "TEMP_DECAY = 0.93\n",
        "\n",
        "# fig, ax = plt.subplots(1, NUM_EPOCHS, figsize=(3*NUM_EPOCHS, 4))\n",
        "scores = []\n",
        "sampled_scores = []\n",
        "\n",
        "with torch.inference_mode(True):\n",
        "\n",
        "  latents = torch.randn(LATENT_SHAPE(1), device=DEVICE, dtype=DTYPE)\n",
        "  image = turbo_pipeline(\n",
        "    prompt=PROMPT,\n",
        "    num_inference_steps=4,\n",
        "    guidance_scale=0.0,\n",
        "    num_images_per_prompt=1,\n",
        "    latents=latents\n",
        "  ).images[0]\n",
        "  score = reward_model.inference_rank(PROMPT, [image])[1]\n",
        "\n",
        "  t = 0\n",
        "  # ax[0].imshow(image)\n",
        "  # ax[0].axis(\"off\")\n",
        "  # ax[0].set_title(f\"Step {t}: r={score:.2f}\")\n",
        "  # scores.append(score)\n",
        "\n",
        "  temp = START_TEMP\n",
        "  mut = MUTATION_START\n",
        "\n",
        "  for epoch in (pbar:=tqdm(range(1, NUM_EPOCHS))):\n",
        "    for step in range(STEPS_PER_EPOCH):\n",
        "\n",
        "      basis = torch.randn(LATENT_SHAPE(BATCH_SIZE), device=DEVICE, dtype=DTYPE)\n",
        "\n",
        "      test_latents = slerp(mut, torch.cat([latents]*BATCH_SIZE,dim=0), basis)\n",
        "      test_images = pipeline(\n",
        "        prompt=PROMPT,\n",
        "        num_inference_steps=1,\n",
        "        guidance_scale=0.0,\n",
        "        num_images_per_prompt=BATCH_SIZE,\n",
        "        latents=test_latents\n",
        "      ).images\n",
        "      test_scores = reward_model.inference_rank(PROMPT, test_images)[1]\n",
        "\n",
        "      best = np.argmax(test_scores)\n",
        "      if test_scores[best] > score:\n",
        "        latents = test_latents[best][None]\n",
        "        image = test_images[best]\n",
        "        score = test_scores[best]\n",
        "\n",
        "      else:\n",
        "        logits = torch.tensor([score]+test_scores) / temp\n",
        "        keeper = torch.distributions.Categorical(logits=logits).sample()\n",
        "\n",
        "        if keeper > 0:\n",
        "          latents = test_latents[keeper-1][None]\n",
        "          image = test_images[keeper-1]\n",
        "          score = test_scores[keeper-1]\n",
        "\n",
        "      pbar.set_postfix(score=f\"{score:.2f}\")\n",
        "      scores.append(score)\n",
        "      sampled_scores.append(test_scores)\n",
        "\n",
        "      temp *= TEMP_DECAY\n",
        "      mut *= MUTATION_DECAY\n",
        "      t += 1\n",
        "\n",
        "    # ax[epoch].imshow(image)\n",
        "    # ax[epoch].axis(\"off\")\n",
        "    # ax[epoch].set_title(f\"Step {t}: r={score:.2f}\")\n",
        "\n",
        "plt.imshow(image)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Stable DIffusion Turbo\")\n",
        "plt.show()\n",
        "\n",
        "# plt.suptitle(f'Image Progress For prompt \"{PROMPT}\"')\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "# plt.clf()\n",
        "\n",
        "# sampled_scores = np.array(sampled_scores)\n",
        "# plt.scatter(\n",
        "#     1+np.arange(len(sampled_scores)).repeat(sampled_scores.shape[1]),\n",
        "#     sampled_scores.reshape(-1)\n",
        "# )\n",
        "# plt.xlabel(\"Sampling Step\")\n",
        "# plt.ylabel(\"Rewards\")\n",
        "# plt.title(f'Rewards For prompt \"{PROMPT}\"')\n",
        "# plt.plot(scores)\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFNGqsm2Diey"
      },
      "outputs": [],
      "source": [
        "\n",
        "NUM_EPOCHS = 5\n",
        "STEPS_PER_EPOCH = 5\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "MUTATION_START = 1 # 1/2\n",
        "MUTATION_DECAY = 1 # 0.95**2\n",
        "\n",
        "START_TEMP = 1/2\n",
        "TEMP_DECAY = 0.93**2\n",
        "\n",
        "with open(\"benchmark-prompts.json\", \"r\") as f:\n",
        "  prompts = json.load(f)\n",
        "\n",
        "rewards = []\n",
        "\n",
        "with torch.inference_mode():\n",
        "\n",
        "  for itm in (pbar:=tqdm(prompts)):\n",
        "    prompt = itm[\"prompt\"]\n",
        "\n",
        "    latents = torch.randn(LATENT_SHAPE(1), device=DEVICE, dtype=DTYPE)\n",
        "    image = pipeline(\n",
        "      prompt=prompt,\n",
        "      num_inference_steps=4,\n",
        "      guidance_scale=0.0,\n",
        "      num_images_per_prompt=1,\n",
        "      latents=latents\n",
        "    ).images[0]\n",
        "    score = reward_model.inference_rank(prompt, [image])[1]\n",
        "\n",
        "    best_score = score\n",
        "\n",
        "    # temp = START_TEMP\n",
        "    # mut = MUTATION_START\n",
        "\n",
        "    # for epoch in tqdm(range(1, NUM_EPOCHS)):\n",
        "    #   for step in range(STEPS_PER_EPOCH):\n",
        "\n",
        "    #     basis = torch.randn(LATENT_SHAPE(BATCH_SIZE), device=DEVICE, dtype=DTYPE)\n",
        "\n",
        "    #     test_latents = slerp(mut, torch.cat([latents]*BATCH_SIZE,dim=0), basis)\n",
        "    #     test_images = pipeline(\n",
        "    #       prompt=prompt,\n",
        "    #       num_inference_steps=1,\n",
        "    #       guidance_scale=0.0,\n",
        "    #       num_images_per_prompt=BATCH_SIZE,\n",
        "    #       latents=test_latents\n",
        "    #     ).images\n",
        "    #     test_scores = reward_model.inference_rank(prompt, test_images)[1]\n",
        "\n",
        "    #     best = np.argmax(test_scores)\n",
        "    #     if test_scores[best] > score:\n",
        "    #       latents = test_latents[best][None]\n",
        "    #       image = test_images[best]\n",
        "    #       score = test_scores[best]\n",
        "\n",
        "    #     else:\n",
        "    #       logits = torch.tensor([score]+test_scores) / temp\n",
        "    #       keeper = torch.distributions.Categorical(logits=logits).sample()\n",
        "\n",
        "    #       if keeper > 0:\n",
        "    #         latents = test_latents[keeper-1][None]\n",
        "    #         image = test_images[keeper-1]\n",
        "    #         score = test_scores[keeper-1]\n",
        "\n",
        "    #     if score > best_score:\n",
        "    #       best_score = score\n",
        "\n",
        "    #     temp *= TEMP_DECAY\n",
        "    #     mut *= MUTATION_DECAY\n",
        "\n",
        "    rewards.append(best_score)\n",
        "    pbar.set_postfix(mean=f\"{np.mean(rewards):.3f}\", std=f\"{np.std(rewards):.3f}\")\n",
        "\n",
        "    np.save(\"rewards.npy\", np.array(rewards))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUNfILulPNqz"
      },
      "outputs": [],
      "source": [
        "print(np.mean(rewards), np.std(rewards))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "a4vpRvg1_kaI",
        "shj79zHO_uQJ",
        "JEk9A5Aw_20i",
        "bVqYrUvIAAZr",
        "CutIl7ENAFut",
        "-0o0UBjnANEj",
        "GpGFu-QtBmpd",
        "-sXr7k8IOPl4",
        "Rpgcp8xwwUj9",
        "RteFh1FLOSO0"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}